# Лабораторная работа №4

В работе изспользуются графики для валидации.  
Базовый алгоритм взят из прошлой лабораторной работы - политика косинусного затухания:
```
learning_rate_cos_res = (
  tf.keras.experimental.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps, t_mul, m_mul))
```
Где initial_learning_rate = 0.001 first_decay_steps=7700 t_mul=1.7 m_mul=0.7.


## 1. С использованием, техники обучения Transfer Learning и оптимальной политики изменения темпа обучения, определенной в ходе выполнения лабораторной #3, обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Food-101 с использованием следующих техник аугментации данных, так же определить оптимальный набор параметров для этих техник аугментации:  

### a. Случайное горизонтальное и вертикальное отражение 

При использовании техники аугментации данных "Случайное горизонтальное и вертикальное отражение" мы взяли команду:  
```
tf.keras.layers.experimental.preprocessing.RandomFlip(
    mode=HORIZONTAL_AND_VERTICAL)
```

### Графики для валидации:
- Оранжевый - базовый алгоритм
- Синий - mode = "horizontal_and_vertical" 
- Красный - mode = "horizontal"
- Голубой - mode = "vertical"

*График точности*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/categorical_accuracy_rand_flip.svg)

*График потерь*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/loss_rand_flip.svg)

*Пример измененного изображения*  
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/img_horizont.jpg)

### b. Использование случайной части изображения 

При использовании техники аугментации данных "Использование случайной части изображения" мы взяли команду:  
```
tf.keras.layers.experimental.preprocessing.RandomCrop(
    height = 224, width = 224)
```

### Графики для валидации:
- Оранжевый - базовый алгоритм
- Синий - начальный размер изображения 400*400 
- Красный - начальный размер изображения 300*300 
- Голубой - начальный размер изображения 250*250 
- Розовый - начальный размер изображения 350*350 

*График точности*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/categorical_accuracy_rand_crop.svg)

*График потерь*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/loss_rand_crop.svg)

*Пример измененного изображения*  
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/img_crop.jpg)

### c. Поворот на случайный угол

При использовании техники аугментации данных "Поворот на случайный угол" мы взяли команду:  
```
tf.keras.layers.experimental.preprocessing.RandomRotation(
    factor)
```  
Где factor - это множитель угла случайного поворота(угол случайного поворота = 2Pi * factor) 

### Графики для валидации:
- Оранжевый - базовый алгоритм
- Синий - factor = 0.5, что дает угол поворота от -180 до 180 градусов
- Красный - factor = 0.25, что дает угол поворота от -90 до 90 градусов
- Голубой - factor = 0.15, что дает угол поворота от -54 до 54 градусов
- Розовый - factor = 0.05, что дает угол поворота от -18 до 18 градусов

*График точности*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/categorical_accuracy_rand_rot.svg)

*График потерь*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/loss_rand_rot.svg)

*Пример измененного изображения*  
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/img.jpg)

## 2. Обучить нейронную сеть с использованием исследованных техник аугментации данных совместно

### Графики обучения:
- Оранжевый - темп 0.001 decay_steps = 7700 на валидации
- Красный - темп 0.01 decay_steps = 7700 на валидации
- Синий - темп 0.001 decay_steps = 66600 на валидации
- Голубой - темп 0.0001 decay_steps = 7700 на валидации
- Розовый - темп 0.0001 decay_steps = 22200 на валидации

*График точности на валидации*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/categorical_accuracy_full.svg)

*График потерь на валидации*
![Alt-текст](https://github.com/the-GriS/CNN-food-101/blob/lab_4/diagrams/lab_4/loss_full.svg)

## Анализ результатов
Если судить по выборке обучения, то лучшим фиксированным темпом обучения является 0.001, т.к точность на обучении достигает 83.77 %, при потерях 0.5468. При этом стоит заметить, что при валидации лучшее значение получается при фиксированном темпе обучения 0.0001, которое равно 67.16% при потерях 1.222. Таким образом можно сделать вывод, что оптимальным темпом обучения является 0.0001.

При использовании политики косинусного затухания initial_learning_rate подбирался(0.01, 0.001, 0.0001), decay_steps - (7700, 66600). Судя по графикам при decay_steps = 7700 и при decay_steps = 22200 наше обучение в определенный момент прерывается. Это связанно с тем, что значение decay_steps меньше количества всех наших итераций и при достижении конца этих итераций learning_rate принимает значение 0, что можно проследить и в графике изменения темпа обучения. Оптимальными параметрами являются initial_learning_rate = 0,001 decay_steps = 7700 ,т. к. при таких значениях получается лучшее значение на валидации равное 66.54% при потерях 1.25.  

При использовании политики косинусного затухания c перезапусками мы приняли initial_learning_rate = 0.001, и в этом случае прослеживалась следующая тенденция: на валидации лучшее значение равное 67.66%  при потерях 1.212 было полученно на 18 эпохе когда first_decay_steps=7700 t_mul=1.7 m_mul=0.7(синий график), а на обучении лучшее значение равное 82.33% при потерях 0.6451 было полученно на 41 эпохе когда first_decay_steps=7700 t_mul=1.3 m_mul=0.9(красный график).
